# Merging, re-shaping, and regular expressions

```{r, include = FALSE}
par(bg = '#fdf6e3')
```

## Seminar

In this part of the course, we will merge data sets again. The difference to the previous exercise is that both data sets contain the same observations but different variables (columns) which is the more common case. We explain how re-shaping data works with a small "toy" example which should make the procedure more clear. Re-shaping is very often necessary for producing ggplot graphs. Finally, we will introduce a simple regular expression example. Regular expressions are extremely useful to identifying similar content that is not exactly identical.

### Merging

We start by loading the `eu` data set again.

<div class="container btn-container">
  <button type = "button" class = "btn btn-lg btn-success" onclick="window.open('qog_eureg_long_sep16.csv');">Download Data</button>
  <button type = "button" class = "btn btn-lg btn-success" onclick="window.open('https://www.qogdata.pol.gu.se/data/qog_eureg_sep16.pdf');">Codebook</button>
</div>

```{r}
eu <- read.csv("qog_eureg_long_sep16.csv", stringsAsFactors = FALSE)
```

We now create a subset of the data set that only includes the variables *year*, *country*, *region*,  *wealth*, *quality.of.government*. We also filter by France, Germany, and the UK, and the years 2000 &ndash; 2013. Furthermore, we must also filter out the observations within a country that are the country itself. Then, we save the data set as `eu_subset1.csv`. 

```{r}
library(tidyverse)

# copy, rename, select, filter, save
eu_subset12 <- eu %>%
  dplyr::select(year,
         country = NUTS0,
         region = region_name,
         wealth = econ_2gdp_eur_hab,
         quality.of.government = eqi_eqi) %>%
  filter(country %in% c("FR", "DE", "UK") & year > 1999 & year < 2014 & 
           !(region %in% c("DEUTSCHLAND ", "FRANCE", "UNITED KINGDOM"))) %>%
  write.table(file = "eu_subset1.csv", sep = ",",
              row.names = FALSE, col.names = TRUE) 
```

Let us create another subset of the original data set. It will include the same countries plus the Netherlands and Belgium but on the aggregate level, i.e. it is not the regional level. Furthermore, there will be an additional variable which is the total population size but it will exclude wealth and the quality of government.

```{r}
eu_subset2 <- eu %>%
    dplyr::select(
      year,
      country = NUTS0,
      population = demo_d2jan_t) %>%
  filter(country %in% c("FR", "DE", "UK") & year > 1999 & year < 2014) %>%
  group_by(country, year) %>%
  summarize(population = sum(population, na.rm = TRUE)) %>%
  write.table(file = "eu_subset2.csv", sep = ",",
              row.names = FALSE, col.names = TRUE)
```

Note that we needed to use the `sum()` function in summarize rather than `mean()` when we wanted average wealth or average quality of government. The *population* variable is the sum of the regional populations in a given year and country. To demonstrate merging, we now clear our workspace.

```{r}
rm(list=ls())
```

We load the two data sets we created and inspect them. We do this to understand what will happen in merging.

```{r}
eu1 <- read.csv(file = "eu_subset1.csv", sep = ",", stringsAsFactors = FALSE)
eu2 <- read.csv(file = "eu_subset2.csv", sep = ",", stringsAsFactors = FALSE)
```

We start by printing the first 30 observations of the `eu1` data set.

```{r}
head(eu1, n = 30)
```

It is clear that the data set is on the region-year level. Let us inspect the second data set in the same way. We print the entire data set because it is much smaller.

```{r}
eu2
```

This data set is on the country-year level. Let's say we want to merge the second data set to the first data set. The second data set includes the variable *population* which will be added to the new combinded data set but the observations are one the country-year level. Therefore, all regions in a country-year will get the same population value (which is the population of the entire country). Because the second data set is more aggregated, we cannot get the population on a more dis-aggregated level but we can still combine both data sets. We do so with the `merge()` function

```{r}
eu3 <- merge(x = eu1, y = eu2, by = c("year", "country"))
dim(eu3)
```
So, the new data set has the same amount of observations as the bigger one but if we check the population variable, it is the same value for all regions within a country-year.

### Re-shaping

Re-shaping a data set is useful, for instance for plotting graphs. For example, have a dependent variable, we an independent variable, and we have different model predictions. Let us create some a small data set with 10 observations and two correlated independent variables.

```{r}
library(MASS)
set.seed(123)

# 2 correlated variables
X <- mvrnorm(n = 10, mu = c(12, -4), Sigma = matrix(data = c(1, 0.8, 0.8, 1), nrow = 2, ncol = 2))
X <- data.frame(x1 = X[,1], x2 = X[,2])
X
```
Now that we have the two independent variables, let's create the outcome variable as a linear function of *x2* and some random noise.

```{r}
X$y <- 1.7 + X$x2 * -2.5 + rnorm(n = 10, mean = 0, sd = 3)
```

With this done, we run three linear models. The first includes *x1* only, the second *x2* only and the third both *x1* and *x2*. We then make prediction of y including confidence intervals for all three models and attach them to the data set.

```{r}
# regressions
m1 <- glm(y ~ x1, family = "gaussian", data = X)
m2 <- glm(y ~ x2, family = "gaussian", data = X)
m3 <- glm(y ~ x1 + x2, family = "gaussian", data = X)

# predictions
preds1 <- predict(m1, se.fit = TRUE)
preds2 <- predict(m2, se.fit = TRUE)
preds3 <- predict(m3, se.fit = TRUE)

# attach point estimates
X$bestguess1 <- preds1$fit
X$bestguess2 <- preds2$fit
X$bestguess3 <- preds3$fit

# attach lower and upper bounds of the 95% CI from a t with the appropriate degrees of freedom
X$lowerbound1 <- preds1$fit - qt(p = 0.975, df = m1$df.residual) * preds1$se.fit
X$lowerbound2 <- preds2$fit - qt(p = 0.975, df = m2$df.residual) * preds2$se.fit
X$lowerbound3 <- preds3$fit - qt(p = 0.975, df = m3$df.residual) * preds3$se.fit
X$upperbound1 <- preds1$fit + qt(p = 0.975, df = m1$df.residual) * preds1$se.fit
X$upperbound2 <- preds2$fit + qt(p = 0.975, df = m2$df.residual) * preds2$se.fit
X$upperbound3 <- preds3$fit + qt(p = 0.975, df = m3$df.residual) * preds3$se.fit

# print data set
X
```

The data set is in the common format for analysis. However, if we want to plot the predictions in ggplot and differentiate them with color by model, we have to re-shape the data set into a long format. In long format, we want to have a new variable called *model* which takes on the values "model 1", "model 2", "model 3" corresponding to the respective model that made the prediction of a best guess, lower or upper bound. We make the data set three times as long by combining *bestguess1*, *bestguess2*, and *bestguess3* into one *bestguess* variable. We do the same with the upper and lower bounds. The variables *x1*, *x2*, and *y* are so called "id" variables because we just recycle the values that are already there, i.e. nothing is combined here. We use the `reshape()` function.

Have a look at the argument's meaning in the help window (usually at lower right in RStudio) by running `?reshape`.


```{r}
eu.long <- reshape(data = X, varying = c("bestguess1", "lowerbound1", "upperbound1",
                                         "bestguess2", "lowerbound2", "upperbound2",
                                         "bestguess3", "lowerbound3", "upperbound3"),
                   timevar = "Model",
                   times = c(1,2,3),
                   v.names = c("bestguess","lowerbound","upperbound"),
                   idvar = c("x1","x2", "y"),
                   direction = "long")

# change the row.names which are always combinations of the idvars
row.names(eu.long) <- seq(1:nrow(eu.long))

# inspect data set
eu.long
```

Ee turn the variable model into a factor variable.

```{r}
eu.long$Model <- factor(eu.long$Model, levels = c(1,2,3), labels = c("Incorrect Model", "Correct Model", "Full Model"))
table(eu.long$Model)
```

We plot using ggplot.

```{r}
ggplot(eu.long, aes(x = x2, y = y)) +
  geom_point() +
  geom_line( aes(y = bestguess, color = Model)) +
  geom_ribbon( aes(ymin = lowerbound, ymax = upperbound, fill = Model), alpha = 0.3)
  
```

We see that the all model predictions overlap. The incorrect model does worst and also has the largest amount of uncertainty. While it is hard to see, the correct model is closest to the real predictions and has the smallest uncertainty. Go ahead and plot the size of the residuals on your own.


### Regular expression

Regular expressions are difficult to master but very powerful when it comes to working with data. Regular expressions can be used to extract email addresses, phone numbers, country names and much more. We provide a simple example and invite you to search online for more complex tasks.

We examine the `eu` data set and the regions in the UK.

```{r}
unique(eu1$region[eu1$country=="UK"])
```

There are multiple regions of London in the data set. Say we wanted to aggregate wealth for all the London regions but we did not want to pick out the regions by hand. This is what regular expressions excel at. We use the `grep()` function which returns the row numbers of all London districts. Let's subset the data set to London only and then compare the wealth of London regions over time in a plot.

```{r}
grep(pattern = "London", x = eu1$region, ignore.case = TRUE)
```

```{r}
london <- eu1 %>%
  slice( grep(pattern = "London", x = eu1$region, ignore.case = TRUE) ) %>%
  filter( region != "LONDON" )

ggplot(london, aes(x = year, y = wealth)) +
       geom_line( aes(y = wealth, color = region)) 
```

It seems like Inner London - West is the place to be.
